{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dense,Dropout,Activation,Conv2D,MaxPooling2D,BatchNormalization,Flatten\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import rmsprop_v2\n",
    "from keras.callbacks import EarlyStopping,ReduceLROnPlateau,ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "Angry\n",
      "Fear\n",
      "Happy\n",
      "Neutral\n",
      "Sad\n",
      "Surprise\n",
      "validation\n",
      "Angry\n",
      "Fear\n",
      "Happy\n",
      "Neutral\n",
      "Sad\n",
      "Surprise\n"
     ]
    }
   ],
   "source": [
    "int2emotions = {0:'Angry',1:'Fear',2:'Happy',3:'Neutral',4:'Sad',5:'Surprise'}\n",
    "emotions2int = {'Angry':0,'Fear':1,'Happy':2,'Neutral':3,'Sad':4,'Surprise':5}\n",
    "\n",
    "dic = {'images':[], 'labels':[], 'purpose':[]}\n",
    "    \n",
    "for d in os.listdir('fer2013/'):\n",
    "    print(d)\n",
    "    for emotion in os.listdir(f'fer2013/{d}'):\n",
    "        print(emotion)\n",
    "        for i in os.listdir(f'fer2013/{d}/{emotion}'):\n",
    "            img = cv2.imread(f'fer2013/{d}/{emotion}/{i}',0)\n",
    "            img = img.reshape(48,48,1)\n",
    "            \n",
    "            dic['images'].append(img)\n",
    "            dic['labels'].append(emotion)\n",
    "            \n",
    "            if d=='train':\n",
    "                dic['purpose'].append('T')\n",
    "            else:\n",
    "                dic['purpose'].append('V')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>images</th>\n",
       "      <th>labels</th>\n",
       "      <th>purpose</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[[72], [78], [81], [75], [59], [54], [63], [6...</td>\n",
       "      <td>Angry</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[[152], [149], [147], [157], [146], [133], [1...</td>\n",
       "      <td>Angry</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[[117], [116], [114], [99], [77], [54], [36],...</td>\n",
       "      <td>Angry</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[[159], [159], [145], [159], [165], [162], [1...</td>\n",
       "      <td>Angry</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[[254], [253], [255], [251], [235], [186], [1...</td>\n",
       "      <td>Angry</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              images labels purpose\n",
       "0  [[[72], [78], [81], [75], [59], [54], [63], [6...  Angry       T\n",
       "1  [[[152], [149], [147], [157], [146], [133], [1...  Angry       T\n",
       "2  [[[117], [116], [114], [99], [77], [54], [36],...  Angry       T\n",
       "3  [[[159], [159], [145], [159], [165], [162], [1...  Angry       T\n",
       "4  [[[254], [253], [255], [251], [235], [186], [1...  Angry       T"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(dic)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data = df[df['purpose']=='T']\n",
    "val_data = df[df['purpose']=='V']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>images</th>\n",
       "      <th>labels</th>\n",
       "      <th>purpose</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[[72], [78], [81], [75], [59], [54], [63], [6...</td>\n",
       "      <td>Angry</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[[152], [149], [147], [157], [146], [133], [1...</td>\n",
       "      <td>Angry</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[[117], [116], [114], [99], [77], [54], [36],...</td>\n",
       "      <td>Angry</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[[159], [159], [145], [159], [165], [162], [1...</td>\n",
       "      <td>Angry</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[[254], [253], [255], [251], [235], [186], [1...</td>\n",
       "      <td>Angry</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              images labels purpose\n",
       "0  [[[72], [78], [81], [75], [59], [54], [63], [6...  Angry       T\n",
       "1  [[[152], [149], [147], [157], [146], [133], [1...  Angry       T\n",
       "2  [[[117], [116], [114], [99], [77], [54], [36],...  Angry       T\n",
       "3  [[[159], [159], [145], [159], [165], [162], [1...  Angry       T\n",
       "4  [[[254], [253], [255], [251], [235], [186], [1...  Angry       T"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>images</th>\n",
       "      <th>labels</th>\n",
       "      <th>purpose</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28273</th>\n",
       "      <td>[[[169], [117], [102], [89], [88], [74], [77],...</td>\n",
       "      <td>Angry</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28274</th>\n",
       "      <td>[[[137], [144], [65], [78], [91], [92], [96], ...</td>\n",
       "      <td>Angry</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28275</th>\n",
       "      <td>[[[8], [14], [8], [35], [148], [223], [241], [...</td>\n",
       "      <td>Angry</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28276</th>\n",
       "      <td>[[[83], [72], [63], [69], [79], [67], [87], [8...</td>\n",
       "      <td>Angry</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28277</th>\n",
       "      <td>[[[75], [80], [80], [80], [82], [92], [64], [2...</td>\n",
       "      <td>Angry</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  images labels purpose\n",
       "28273  [[[169], [117], [102], [89], [88], [74], [77],...  Angry       V\n",
       "28274  [[[137], [144], [65], [78], [91], [92], [96], ...  Angry       V\n",
       "28275  [[[8], [14], [8], [35], [148], [223], [241], [...  Angry       V\n",
       "28276  [[[83], [72], [63], [69], [79], [67], [87], [8...  Angry       V\n",
       "28277  [[[75], [80], [80], [80], [82], [92], [64], [2...  Angry       V"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Happy       7215\n",
       "Neutral     4965\n",
       "Sad         4830\n",
       "Fear        4097\n",
       "Angry       3995\n",
       "Surprise    3171\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Happy       879\n",
       "Neutral     626\n",
       "Sad         594\n",
       "Fear        528\n",
       "Angry       491\n",
       "Surprise    416\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>images</th>\n",
       "      <th>labels</th>\n",
       "      <th>purpose</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[[148], [163], [173], [175], [138], [122], [1...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[[58], [43], [50], [62], [57], [49], [43], [3...</td>\n",
       "      <td>Angry</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[[22], [0], [5], [17], [0], [16], [119], [229...</td>\n",
       "      <td>Angry</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[[191], [176], [128], [137], [134], [145], [1...</td>\n",
       "      <td>Surprise</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[[193], [192], [194], [194], [205], [212], [2...</td>\n",
       "      <td>Sad</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              images    labels purpose\n",
       "0  [[[148], [163], [173], [175], [138], [122], [1...   Neutral       T\n",
       "1  [[[58], [43], [50], [62], [57], [49], [43], [3...     Angry       T\n",
       "2  [[[22], [0], [5], [17], [0], [16], [119], [229...     Angry       T\n",
       "3  [[[191], [176], [128], [137], [134], [145], [1...  Surprise       T\n",
       "4  [[[193], [192], [194], [194], [205], [212], [2...       Sad       T"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "happy_df = train_data[train_data['labels']=='Happy'].sample(n=3171)\n",
    "neutral_df = train_data[train_data['labels']=='Neutral'].sample(n=3171)\n",
    "sad_df = train_data[train_data['labels']=='Sad'].sample(n=3171)\n",
    "fear_df = train_data[train_data['labels']=='Fear'].sample(n=3171)\n",
    "angry_df = train_data[train_data['labels']=='Angry'].sample(n=3171)\n",
    "surprise_df = train_data[train_data['labels']=='Surprise'].sample(n=3171)\n",
    "\n",
    "train_data = pd.concat([happy_df,neutral_df,sad_df,fear_df,angry_df,surprise_df])\n",
    "\n",
    "train_data = train_data.sample(frac=1)\n",
    "train_data.reset_index(inplace=True)\n",
    "train_data.drop('index',inplace=True,axis=1)\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sad         3171\n",
       "Angry       3171\n",
       "Happy       3171\n",
       "Surprise    3171\n",
       "Fear        3171\n",
       "Neutral     3171\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='labels', ylabel='count'>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(train_data['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size= 32\n",
    "classes = 6\n",
    "rows,columns=48,48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = list(train_data['labels'].replace(emotions2int))\n",
    "train_labels = to_categorical(train_labels)\n",
    "\n",
    "val_labels = list(val_data['labels'].replace(emotions2int))\n",
    "val_labels = to_categorical(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = list(train_data['images'])\n",
    "train_data = np.array(train_data)\n",
    "\n",
    "val_data = list(val_data['images'])\n",
    "val_data = np.array(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19026, 48, 48, 1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3534, 48, 48, 1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size= 32\n",
    "# classes = 6\n",
    "# rows,columns=48,48\n",
    "\n",
    "# train_dir = 'fer2013\\\\train' \n",
    "# validation_dir = 'fer2013\\\\validation'\n",
    "\n",
    "# train_gen = ImageDataGenerator(rescale=1./255,\n",
    "#                               rotation_range=45,\n",
    "#                               height_shift_range=0.3,\n",
    "#                               width_shift_range=0.3,\n",
    "#                               shear_range=0.2,\n",
    "#                               horizontal_flip=True,\n",
    "#                               fill_mode='nearest')\n",
    "\n",
    "# validation_gen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# training_data_generator = train_gen.flow_from_directory(train_dir,\n",
    "#                                                        batch_size=batch_size,\n",
    "#                                                        shuffle=True,\n",
    "#                                                        color_mode='grayscale',\n",
    "#                                                        class_mode='categorical',\n",
    "#                                                        target_size=(rows,columns))\n",
    "\n",
    "# valiadtion_data_generator = validation_gen.flow_from_directory(validation_dir,\n",
    "#                                                        batch_size=batch_size,\n",
    "#                                                        shuffle=False,\n",
    "#                                                        color_mode='grayscale',\n",
    "#                                                        class_mode='categorical',\n",
    "#                                                        target_size=(rows,columns))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 48, 48, 64)        640       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 48, 48, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 48, 48, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 48, 48, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 24, 24, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 24, 24, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 24, 24, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 24, 24, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 12, 12, 256)       295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 12, 12, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 12, 12, 256)       590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 12, 12, 256)       1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 6, 6, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 6, 6, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 6, 6, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 6, 6, 512)         2048      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 3, 3, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 3, 3, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               1179904   \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 5,915,142\n",
      "Trainable params: 5,910,406\n",
      "Non-trainable params: 4,736\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# First Block\n",
    "model.add(Conv2D(64,(3,3),activation='elu',input_shape=(rows,columns,1),kernel_initializer='he_normal',padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64,(3,3),activation='elu',input_shape=(rows,columns,1),kernel_initializer='he_normal',padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Second Block\n",
    "model.add(Conv2D(128,(3,3),activation='elu',kernel_initializer='he_normal',padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(128,(3,3),activation='elu',kernel_initializer='he_normal',padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Third Block\n",
    "model.add(Conv2D(256,(3,3),activation='elu',kernel_initializer='he_normal',padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(256,(3,3),activation='elu',kernel_initializer='he_normal',padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Fourth Block\n",
    "model.add(Conv2D(512,(3,3),activation='elu',kernel_initializer='he_normal',padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(512,(3,3),activation='elu',kernel_initializer='he_normal',padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Fifth Block\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256,activation='elu',kernel_initializer='he_normal'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Sixth Block\n",
    "model.add(Dense(128,activation='elu',kernel_initializer='he_normal'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Seventh Block\n",
    "model.add(Dense(64,activation='elu',kernel_initializer='he_normal'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Eighth Block\n",
    "model.add(Dense(classes,activation='softmax',kernel_initializer='he_normal'))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint('model\\\\6_class_emotion_detector_V2.h5',\n",
    "                             save_best_only=True,\n",
    "                             mode='min',\n",
    "                             monitor='val_loss',\n",
    "                             verbose=1)\n",
    "\n",
    "earlystopping = EarlyStopping(patience=10,\n",
    "                             verbose=1,\n",
    "                             min_delta=0,\n",
    "                             monitor='val_loss',\n",
    "                             restore_best_weights=True)\n",
    "\n",
    "\n",
    "callbacks = [checkpoint, earlystopping]\n",
    "\n",
    "model.compile(metrics=['accuracy'],\n",
    "             optimizer='rmsprop',\n",
    "             loss='categorical_crossentropy')\n",
    "\n",
    "train_samples = 28273\n",
    "validation_samples = 3534\n",
    "batch_size = 64\n",
    "epochs=30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "441/441 [==============================] - 67s 105ms/step - loss: 2.4345 - accuracy: 0.1823 - val_loss: 1.8593 - val_accuracy: 0.2281ss: 2.441\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.85933, saving model to model\\6_class_emotion_detector_V2.h5\n",
      "Epoch 2/30\n",
      "441/441 [==============================] - 44s 100ms/step - loss: 1.7370 - accuracy: 0.2698 - val_loss: 1.5590 - val_accuracy: 0.3831oss: 1\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.85933 to 1.55897, saving model to model\\6_class_emotion_detector_V2.h5\n",
      "Epoch 3/30\n",
      "441/441 [==============================] - 44s 100ms/step - loss: 1.4838 - accuracy: 0.3964 - val_loss: 1.5069 - val_accuracy: 0.4270\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.55897 to 1.50687, saving model to model\\6_class_emotion_detector_V2.h5\n",
      "Epoch 4/30\n",
      "441/441 [==============================] - 45s 102ms/step - loss: 1.3203 - accuracy: 0.4815 - val_loss: 1.4112 - val_accuracy: 0.4570\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.50687 to 1.41115, saving model to model\\6_class_emotion_detector_V2.h5\n",
      "Epoch 5/30\n",
      "441/441 [==============================] - 45s 102ms/step - loss: 1.2400 - accuracy: 0.5207 - val_loss: 1.3587 - val_accuracy: 0.4994\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.41115 to 1.35871, saving model to model\\6_class_emotion_detector_V2.h5\n",
      "Epoch 6/30\n",
      "441/441 [==============================] - 44s 100ms/step - loss: 1.1692 - accuracy: 0.5500 - val_loss: 1.3683 - val_accuracy: 0.5207\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.35871\n",
      "Epoch 7/30\n",
      "441/441 [==============================] - 44s 100ms/step - loss: 1.1047 - accuracy: 0.5777 - val_loss: 1.4137 - val_accuracy: 0.5144\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.35871\n",
      "Epoch 8/30\n",
      "441/441 [==============================] - 47s 106ms/step - loss: 1.0264 - accuracy: 0.6175 - val_loss: 1.4424 - val_accuracy: 0.5198\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.35871\n",
      "Epoch 9/30\n",
      "441/441 [==============================] - 45s 103ms/step - loss: 0.9633 - accuracy: 0.6439 - val_loss: 1.4911 - val_accuracy: 0.5354\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.35871\n",
      "Epoch 10/30\n",
      "441/441 [==============================] - 45s 102ms/step - loss: 0.8602 - accuracy: 0.6902 - val_loss: 1.5004 - val_accuracy: 0.5306\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.35871\n",
      "Epoch 11/30\n",
      "441/441 [==============================] - 45s 103ms/step - loss: 0.7719 - accuracy: 0.7283 - val_loss: 1.5959 - val_accuracy: 0.5323\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.35871\n",
      "Epoch 12/30\n",
      "441/441 [==============================] - 46s 104ms/step - loss: 0.7067 - accuracy: 0.7582 - val_loss: 1.7697 - val_accuracy: 0.5405\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 1.35871\n",
      "Epoch 13/30\n",
      "441/441 [==============================] - 45s 102ms/step - loss: 0.6299 - accuracy: 0.7863 - val_loss: 1.8730 - val_accuracy: 0.5396\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 1.35871\n",
      "Epoch 14/30\n",
      "441/441 [==============================] - 45s 102ms/step - loss: 0.5385 - accuracy: 0.8201 - val_loss: 2.0165 - val_accuracy: 0.5311\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 1.35871\n",
      "Epoch 15/30\n",
      "441/441 [==============================] - 44s 100ms/step - loss: 0.4990 - accuracy: 0.8362 - val_loss: 2.1729 - val_accuracy: 0.5351\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 1.35871\n",
      "Epoch 16/30\n",
      "441/441 [==============================] - 44s 100ms/step - loss: 0.4147 - accuracy: 0.8622 - val_loss: 2.2819 - val_accuracy: 0.5306\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 1.35871\n",
      "Epoch 17/30\n",
      "441/441 [==============================] - 45s 102ms/step - loss: 0.3770 - accuracy: 0.8758 - val_loss: 2.2751 - val_accuracy: 0.5232\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 1.35871\n",
      "Epoch 18/30\n",
      "441/441 [==============================] - 46s 104ms/step - loss: 0.3327 - accuracy: 0.8927 - val_loss: 2.3443 - val_accuracy: 0.5368\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 1.35871\n",
      "Epoch 19/30\n",
      "441/441 [==============================] - 44s 100ms/step - loss: 0.2970 - accuracy: 0.9046 - val_loss: 2.4195 - val_accuracy: 0.5308\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 1.35871\n",
      "Epoch 20/30\n",
      "441/441 [==============================] - 45s 102ms/step - loss: 0.2959 - accuracy: 0.9085 - val_loss: 2.5389 - val_accuracy: 0.5175\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 1.35871\n",
      "Epoch 21/30\n",
      "441/441 [==============================] - 45s 103ms/step - loss: 0.2503 - accuracy: 0.9216 - val_loss: 2.4581 - val_accuracy: 0.5328\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 1.35871\n",
      "Epoch 22/30\n",
      "441/441 [==============================] - 44s 100ms/step - loss: 0.2248 - accuracy: 0.9312 - val_loss: 2.7538 - val_accuracy: 0.5467\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 1.35871\n",
      "Epoch 23/30\n",
      "441/441 [==============================] - 45s 102ms/step - loss: 0.2407 - accuracy: 0.9282 - val_loss: 2.7193 - val_accuracy: 0.5246\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 1.35871\n",
      "Epoch 24/30\n",
      "441/441 [==============================] - 44s 100ms/step - loss: 0.2242 - accuracy: 0.9328 - val_loss: 2.6998 - val_accuracy: 0.5399\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 1.35871\n",
      "Epoch 25/30\n",
      "441/441 [==============================] - 45s 102ms/step - loss: 0.1839 - accuracy: 0.9452 - val_loss: 2.7879 - val_accuracy: 0.5348\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 1.35871\n",
      "Epoch 26/30\n",
      "441/441 [==============================] - 45s 102ms/step - loss: 0.1967 - accuracy: 0.9426 - val_loss: 2.7746 - val_accuracy: 0.5351\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 1.35871\n",
      "Epoch 27/30\n",
      "441/441 [==============================] - 45s 103ms/step - loss: 0.1774 - accuracy: 0.9462 - val_loss: 2.7664 - val_accuracy: 0.5402\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 1.35871\n",
      "Epoch 28/30\n",
      "441/441 [==============================] - 45s 102ms/step - loss: 0.1648 - accuracy: 0.9511 - val_loss: 2.7768 - val_accuracy: 0.5306\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 1.35871\n",
      "Epoch 29/30\n",
      "441/441 [==============================] - 44s 100ms/step - loss: 0.1644 - accuracy: 0.9489 - val_loss: 2.9112 - val_accuracy: 0.5345\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 1.35871\n",
      "Epoch 30/30\n",
      "201/441 [============>.................] - ETA: 23s - loss: 0.1486 - accuracy: 0.9559- ETA: 25s - loss: 0WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 13230 batches). You may need to use the repeat() function when building your dataset.\n",
      "441/441 [==============================] - 22s 51ms/step - loss: 0.1564 - accuracy: 0.9548 - val_loss: 2.6819 - val_accuracy: 0.5359\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 1.35871\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_data,\n",
    "                    train_labels,\n",
    "                    epochs=epochs,\n",
    "                    steps_per_epoch=train_samples//batch_size,\n",
    "                    validation_data=(val_data,val_labels),\n",
    "                    validation_steps=validation_samples//batch_size,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Live Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-23-3171db864daa>:7: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if faces==():\n"
     ]
    }
   ],
   "source": [
    "model = load_model('model\\\\6_class_emotion_detector_V2.h5')\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "classifier = cv2.CascadeClassifier('Haarcascades\\\\haarcascade_frontalface_default.xml')\n",
    "\n",
    "def detect_face(frame):\n",
    "    faces=classifier.detectMultiScale(frame,1.3,4)\n",
    "    if faces==():\n",
    "        return frame\n",
    "    for x,y,w,h in faces:\n",
    "        cv2.rectangle(frame,(x,y),(x+w,y+h),(172,42,251),2)\n",
    "        face = frame[y:y+h,x:x+w]\n",
    "        face = cv2.cvtColor(face,cv2.COLOR_BGR2GRAY)\n",
    "        face = cv2.resize(face,(48,48))\n",
    "        face = face.reshape(1,48,48,1)\n",
    "        cv2.putText(frame,text=int2emotions[np.argmax(model.predict(face))],\n",
    "                    org=(x,y-15),fontFace=cv2.FONT_HERSHEY_SIMPLEX,fontScale=1,color=(106,40,243),thickness=2)\n",
    "    return frame\n",
    "\n",
    "while 1:\n",
    "    ret,frame= cap.read()\n",
    "    if ret==True:\n",
    "        cv2.imshow('emotion_detector',detect_face(frame))\n",
    "        if cv2.waitKey(1)==27:\n",
    "            break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
